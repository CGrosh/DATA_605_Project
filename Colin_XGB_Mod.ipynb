{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data and initialize the Dask Client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'client' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Intro data set and problem \n",
    "# Each go through model \n",
    "# Do diagram \n",
    "# Do 4 Vs, 1 V each \n",
    "\n",
    "\n",
    "#imports\n",
    "import dask, joblib\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "import dask.array as da\n",
    "from dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler\n",
    "from dask_ml import decomposition, linear_model, metrics\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.model_selection import train_test_split, KFold\n",
    "from dask_ml.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn import tree, ensemble, svm, naive_bayes, neighbors, cluster\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, \n",
    "    confusion_matrix, recall_score, \n",
    "    plot_confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt \n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def cat_features(dataframe):\n",
    "    td = pd.DataFrame({'a': [1,2,3], 'b': [1.0, 2.0, 3.0]})\n",
    "    return filter(lambda x: not(dataframe[x].dtype in [td['a'].dtype, td['b'].dtype]), list(dataframe))\n",
    "\n",
    "#close existing dask connection if it exists and open a new one\n",
    "try:\n",
    "    if client is not None:\n",
    "        client.close()\n",
    "        print(\"closed existing connection, \",client)       \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgrosh/anaconda3/lib/python3.7/site-packages/distributed/dashboard/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36091\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:35657/status' target='_blank'>http://127.0.0.1:35657/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:36091' processes=2 cores=4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get new connection\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='8GB')\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and concatenate the dask dataframes of multiple data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  2097150\n",
      "Number of Columns:  26\n",
      "\n",
      "\n",
      "Length of Test Dataset: 1048574\n",
      "Number of Columns:  26\n"
     ]
    }
   ],
   "source": [
    "# Declaring the \n",
    "\n",
    "feat_types_check = {'Dst Port': float, 'Protocol': int, 'Timestamp': str, \n",
    "                    'Flow Duration': int, 'Tot Fwd Pkts': int, 'Tot Bwd Pkts': int, \n",
    "                    'TotLen Fwd Pkts': int, 'TotLen Bwd Pkts': int, 'Flow Pkts/s': float,  \n",
    "                    'Fwd PSH Flags': int, 'Bwd PSH Flags': int, 'Fwd URG Flags': int, \n",
    "                    'Bwd URG Flags': int, 'Fwd Pkts/s': float, 'Bwd Pkts/s': float,\n",
    "                    'FIN Flag Cnt': int, 'SYN Flag Cnt': int, 'RST Flag Cnt': int, \n",
    "                    'PSH Flag Cnt': int, 'ACK Flag Cnt': int, 'URG Flag Cnt': int, \n",
    "                    'CWE Flag Count': int, 'ECE Flag Cnt': int, 'Subflow Fwd Pkts': int, \n",
    "                    'Subflow Fwd Byts': int, 'Subflow Bwd Pkts': int, 'Subflow Bwd Byts': int, \n",
    "                    'Label': str}\n",
    "\n",
    "feature_cols = ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', \n",
    "                'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts',\n",
    "                'TotLen Bwd Pkts', 'Flow IAT Mean', 'Fwd PSH Flags', \n",
    "                'Bwd PSH Flags', 'Fwd URG Flags','Bwd URG Flags', \n",
    "                'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', \n",
    "                'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
    "                'CWE Flag Count', 'ECE Flag Cnt', 'Subflow Fwd Pkts',\n",
    "                'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', \n",
    "                'Label']\n",
    "\n",
    "# Multiple tests had been done on the differnt files we had available to us \n",
    "\n",
    "df_1 = dd.read_csv('cic_data/14_02_2018.csv', usecols=feature_cols, low_memory=False)\n",
    "df_2 = dd.read_csv('cic_data/15_02_2018.csv', usecols=feature_cols, low_memory=False)\n",
    "test_df = dd.read_csv('cic_data/16_02_2018_fixeddata.csv', usecols=feature_cols, low_memory=False)\n",
    "\n",
    "# test_df = dd.read_csv('cic_data/23_02_2018.csv', usecols=feature_cols, low_memory=False)\n",
    "# test_df = dd.read_csv('cic_data/22_02_2018.csv', usecols=feature_cols, low_memory=False)\n",
    "\n",
    "df = df_1.append(df_2)\n",
    "# df_1 = df_1.append(df_2)\n",
    "# df = df[df['Label'].isin(['Brute Force -Web', 'Brute Force -XSS', 'Benign', 'FTP-BruteForce', 'SSH-BruteForce'])]\n",
    "\n",
    "# test_df = test_df.append(test_df_2)\n",
    "# test_df = test_df[test_df['Label'].isin(['Brute Force -Web', 'Brute Force -XSS', 'Benign', 'FTP-BruteForce', 'SSH-BruteForce'])]\n",
    "\n",
    "print(\"Length of Dataset: \", len(df))\n",
    "print(\"Number of Columns: \", len(df.columns))\n",
    "print('\\n')\n",
    "print(\"Length of Test Dataset:\", len(test_df))\n",
    "print(\"Number of Columns: \", len(test_df.columns))\n",
    "# print(\"Categorical Features: \", list(cat_features(df)))\n",
    "# Files in the zipped folder:\n",
    "# 23_02_2018.csv, 20_02_2018.csv, 16_02_2018.csv, 02_03_2018.csv\n",
    "# with ZipFile('cic_data.zip') as zipped:\n",
    "#     df = dd.from_pandas(pd.read_csv(zipped.open('23_02_2018.csv'), usecols=feature_cols), npartitions=2)\n",
    "#     df = df.append(pd.read_csv(zipped.open('16_02_2018.csv'), usecols=feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Target Variable by Declaring a new column of 1 if the Label Column does not equal 'Benign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'] = 0 \n",
    "df['Target'] = df['Target'].mask(df['Label'] != 'Benign', 1)\n",
    "\n",
    "test_df['Target'] = 0\n",
    "test_df['Target'] = test_df['Target'].mask(test_df['Label'] != 'Benign', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Target']\n",
    "x = df.drop(['Label', 'Target', 'Timestamp'], axis=1)\n",
    "\n",
    "test_y = test_df['Target']\n",
    "test_x = test_df.drop(['Label', 'Target', 'Timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Train Test Split and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes X, Y and gives you a train test split\n",
    "def doTrainTestSplit(X,Y):\n",
    "    #breakpoint()\n",
    "    #look at dask dataframes\n",
    "#     display(X.head())\n",
    "#     display(Y.head())\n",
    "   \n",
    "    #get splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.3)\n",
    "#     display(X_train.compute())\n",
    "#     display(y_train.compute())\n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doKFolds(X,Y):\n",
    "    \n",
    "    #look at dask dataframes\n",
    "    display(X.head())\n",
    "    display(yD.head())\n",
    "\n",
    "    #create KFold object\n",
    "    c = KFold()\n",
    "    #breakpoint()\n",
    "    #split on dask arrays, doesn't work on dataframes yet\n",
    "    gen = c.split(X.to_dask_array(lengths=True),Y.to_dask_array(lengths=True))\n",
    "\n",
    "    #inspect generator\n",
    "    print(gen)\n",
    "    display(type(gen))\n",
    "    \n",
    "    #call generator\n",
    "    for train,test in gen:\n",
    "        print(\"train = \",train.compute())\n",
    "        print(\"test = \",test.compute())\n",
    "        print(\"x train = \",X.loc[train])\n",
    "        got = X.loc[train.compute()]\n",
    "        display(got.head())\n",
    "        #print(got.compute())\n",
    "        clf.fit(got,got)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = doTrainTestSplit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_metrics(mod, feats, labels):\n",
    "    preds = mod.predict(feats.compute())\n",
    "    labels = labels.compute()\n",
    "    print(\"Precision: \", precision_score(labels, preds))\n",
    "    print(\"Accuracy: \", accuracy_score(labels, preds))\n",
    "    print(\"Recall: \", recall_score(labels, preds))\n",
    "    print(\"Confusion Matrix: \", confusion_matrix(labels, preds))\n",
    "\n",
    "# def cluster_eval(mod, feats, labels):\n",
    "#     preds = mod.predict(feats.compute())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the classifiers, multiple were first test but in my code I settled on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main XGboost model used in my code \n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=5, max_depth=6,\n",
    "    sampling_method='uniform', \n",
    "    tree_method='hist', objective='binary:logistic', \n",
    "    booster='dart'\n",
    ")\n",
    "# print(np.sum(x_train.compute().memory_usage())/1e6)\n",
    "\n",
    "# Alternate models originally tested \n",
    "bayes_clf = naive_bayes.GaussianNB()\n",
    "multibayes_clf = naive_bayes.MultinomialNB()\n",
    "rf_clf= tree.DecisionTreeClassifier()\n",
    "knn_clf = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "cluster_clf = cluster.KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(booster='dart', max_depth=6, n_estimators=5,\n",
       "              sampling_method='uniform', tree_method='hist')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(x_train.compute(), y_train.compute(), eval_metric=\"auc\")\n",
    "# bayes_clf.fit(x_train.compute(), y_train.compute())\n",
    "# rf_clf.fit(x_train.compute(), y_train.compute())\n",
    "# cluster_clf.fit(x_train.compute())\n",
    "# knn_clf.fit(x_train.compute(), y_train.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.994872314394146\n",
      "Accuracy:  0.9956057502715547\n",
      "Recall:  0.983809318311422\n",
      "Confusion Matrix:  [[497442    658]\n",
      " [  2101 127665]]\n"
     ]
    }
   ],
   "source": [
    "error_metrics(xgb_clf, x_test, y_test)\n",
    "# error_metrics(bayes_clf, x_test, y_test)\n",
    "# error_metrics(rf_clf, x_test, y_test)\n",
    "# error_metrics(knn_clf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display relevant error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9472793130661774\n",
      "Accuracy:  0.5525532771173041\n",
      "Recall:  0.23336080637817755\n",
      "Confusion Matrix:  [[438956   7816]\n",
      " [461365 140437]]\n"
     ]
    }
   ],
   "source": [
    "error_metrics(xgb_clf, test_x, test_y)\n",
    "# error_metrics(bayes_clf, test_x, test_y)\n",
    "# error_metrics(rf_clf, test_x, test_y)\n",
    "# error_metrics(knn_clf, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.show(plot_confusion_matrix(xgb_clf, test_x.compute(), test_y.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show(plot_confusion_matrix(bayes_clf, test_x.compute(), test_y.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(test_y.compute(), bayes_clf.predict(test_x.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing for comparision between the Pandas and the Dask Threading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 10\n",
    "# #time with just pandas\n",
    "# with joblib.parallel_backend('threading'):\n",
    "#      xgb_clf.fit(x_train.compute(), y_train.compute(), eval_metric=precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 10\n",
    "# #time with just pandas\n",
    "# with joblib.parallel_backend('dask'):\n",
    "#      xgb_clf.fit(x_train.compute(), y_train.compute(), eval_metric=precision_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaa9d3b5d186f149eeb25c9be13305630c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
